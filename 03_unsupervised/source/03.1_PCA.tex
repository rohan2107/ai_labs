\documentclass[12pt]{article}
\usepackage{amsfonts, epsfig}
\usepackage[authoryear]{natbib}
\usepackage{array}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\pagestyle{fancy}
\lfoot{\texttt{ematm0067.github.io} }
\lhead{Introduction to AI - 03.1 PCA - Conor}
\rhead{\thepage}
\cfoot{}

\usepackage{tikz}
\usetikzlibrary{positioning}

\usepackage{ifthen}
\newboolean{nopics}
\setboolean{nopics}{true}


\begin{document}

\section*{Principal Component Analysis.} 

Linear algebra is one of the great superpowers! A lot of what we do in
data science, indeed, a lot of mathematical modelling is either a
direct example of linear algebra, or, something more complicated we
manage to solve because we can break it up into bits that look like
linear algebra.

In this section we are going to look at principal component analysis
(PCA). We have a few reasons to do this; it is a useful example of
unsupervised learning, it reminds us a bit about linear algebra and it
teaches us a bit about how to think about data. In fact, we won't go
into this, but many interesting aspects of neural networks are likely
understandable as non-linear generalizations of PCA!

The first thing is \textsl{unsupervised learning}; by unsupervised
learning we mean, broadly, we don't start with a set of data points
and some labels and try to learn how to map from data to labels,
that's supervised learning; in unsupervised learning we have some data
points and we try to decide what structure the points have, what is
interesting about them, before actually thinking about labelling. Old
fashioned text books make a big deal about the distinction between
supervised and unsupervised learning; often, in real life, we find
that distinction is not so clear; a neural network, for example, is
typically trained on labelled data, so they are exampled of supervised
systems, but often earlier layers do something more akin to
unsupervised learning, breaking input down by its intrinsic structure,
all the better to be classified by later layer.

I am overly inclined to talk about things in terms of what the brain
does, but the brain clearly performs some mixture of unsupervised and
supervised learning. A baby, when it first is learning to see, when
its visual system is wiring itself up, is being presented with visual
data, this data has structure, there are edges, the edges surround
objects, objects appear in front of each other in a sort of
consistient way, they have shading; the same object can be large or
small according to how far away or how close it is. This the baby
learns before it starts to learn to recognize different things; that
is the sort of learning we call unsupervised.

\subsection*{Some linear algebra!}

I don't want to spend ages reminding you how linear algebra works; you
can look this up. However, it might be useful to have a quick
recap. There are lots of ways to think about what matrices do, in
fact, it is a clue to what a powerful concept they are that they can
be introduced in some many different ways!

Here is one version! Consider the simulateous equations:
\begin{equation}
\begin{aligned}
x + y &= 3 \\
x - y &= 1
\end{aligned}
\end{equation}
We can solve this easily by adding the two equation, this give $2x=4$
or $x=2$, substituting that back in gives $y=1$ and it is easy to
check that this solve both equations. The geometrical explanation is
that both equations are equations for lines and the point $(2,1)$ is
the unique point that lies on both lines solving both equations.

We can rewrite this in matrix form:
\begin{equation}
\begin{pmatrix}
1 & 1 \\
1 & -1
\end{pmatrix}
\begin{pmatrix}
x \\ y
\end{pmatrix}
=
\begin{pmatrix}
3 \\ 1
\end{pmatrix}
\end{equation}
which is in the matrix form
\begin{equation}
  A\mathbf{x}=\mathbf{y}
\end{equation}
Provided we can find an inverse matrix $A^{-1}$ so that $A^{-1}A=\mathbf{1}$ then we can solve the equation:
\begin{equation}
  A^{-1}A\mathbf{x}=A^{-1}\mathbf{y}
\end{equation}
or
\begin{equation}
 \mathbf{x}=A^{-1}\mathbf{y}
\end{equation}
In fact if
\begin{equation}
  A=\begin{pmatrix}
1 & 1 \\
1 & -1
  \end{pmatrix}
\end{equation}
as above, then
\begin{equation}
  A^{-1}
  =
\frac12
\begin{pmatrix}
1 & 1 \\
1 & -1
\end{pmatrix}
\end{equation}
or
\begin{equation}
\begin{pmatrix}
x \\ y
\end{pmatrix}
=
\frac12
\begin{pmatrix}
1 & 1 \\
1 & -1
\end{pmatrix}
\begin{pmatrix}
3 \\ 1
\end{pmatrix}
=
\begin{pmatrix}
2 \\ 1
\end{pmatrix}
\end{equation}

So what did that palaver with the matrix get us? In a way it deskilled
the solving of the simultaneous equation; the first time around, when
we solved it without using matrices we began by adding the two
equations so the $y$ terms cancelled and we got a value for $x$. This
didn't require much skill since it was a very easy example, but you
can see that some thinking is required. The matrix method, in
contrast, only relied on the step I sort of skipped, inverting the
matrix; if you write the equation in terms of matrices then you can
solve them by inverting the matrix. In fact, any method of solving the
matrix is equivalent to inverting the matrix, so solving the equation
is exactly as hard as inverting the matrix. We have, over the last 100
years put a lot of effort into finding ways to invert matrices, it is
a long-winded calculation, but one we are very good at, and by ``we''
I mean linear algebra packages.

Not all matrices can be inverted: this matrix
\begin{equation}
  A=\begin{pmatrix}
2 & 3 \\
-4 & -6
  \end{pmatrix}
\end{equation}
for example, has no inverse. That's ok though because not all
simultaneous equations can be solved, for example
\begin{equation}
\begin{aligned}
2x + 3y &= 3 \\
-4x - 6y &= 1
\end{aligned}
\end{equation}
has no solution because the two lines, the $2x+3y=3$ line and the
$-4x-6y=1$ line are parallel. In fact, I can tell straight away that
the matrix can't be inverted by working out its determinant. The determinant is a quantity associated with matrices, in the $2\times 2$ case it has a simple formula:
\begin{equation}
  \det{\begin{pmatrix}
a & c \\
d & b
  \end{pmatrix}}=ab-cd
\end{equation}
So
\begin{equation}
  \det{\begin{pmatrix}
1 & 1 \\
1 & -1
  \end{pmatrix}}=-2
\end{equation}
and
\begin{equation}
  \det{\begin{pmatrix}
2 & 3 \\
-4 & -6
  \end{pmatrix}}=0
\end{equation}
The rule is that a matrix has an inverse if its determinant isn't
zero. The formula for working out the determinant of bigger matrices
is tedious to explain, but easily programmed, we can easily work out determinants, though for anything bigger than $2\times 2$ or certainly $3\times 3$ it's best to get a computer to do it for you!

\subsection*{Eigenvectors and eigenvalues}

Ok so this bit is the special secret bit. Matrices can be thought of
as being made up of their eigenvectors. The word \textsl{eigen} means
equal in German and an eigenvector for a matrix is a direction that isn't changed by the matrix, so it satisfies:
\begin{equation}
  A\textbf{e}=\lambda \textbf{e}
\end{equation}
In other words, multiplying the eigenvector by the matrix can make it
longer or shorter, according to whether $\lambda$ is bigger or smaller
than one, but it stays pointing in the same direction. It isn't clear
yet why this is a big deal, but it will turn out to be a nice way of
thinking how matrices work and we'll use it to do PCA!

How do you find the eigenvectors? Well this is a lovely piece of using
mathematics in a kind of wrong way around trick. We want
\begin{equation}
  A\textbf{e}=\lambda \textbf{e}
\end{equation}
so we can write that as
\begin{equation}
  \left(A-\lambda \textbf{1}\right)\mathbf{e}=\mathbf{0}
\end{equation}
In our simple example from earlier imagine we want
\begin{equation}
  \begin{pmatrix}
1 & 1 \\
1 & -1
\end{pmatrix}\begin{pmatrix}
e_1 \\ e_2
\end{pmatrix}
=\lambda
\begin{pmatrix}
e_1 \\ e_2
\end{pmatrix}
\end{equation}
then we rewrite this as
\begin{equation}
  \begin{pmatrix}
1-\lambda & 1 \\
1 & -1-\lambda
\end{pmatrix}\begin{pmatrix}
e_1 \\ e_2
\end{pmatrix}
=
\begin{pmatrix}
0 \\ 0
\end{pmatrix}
\end{equation}

Back though to the general version
\begin{equation}
  \left(A-\lambda \textbf{1}\right)\mathbf{e}=\mathbf{0}
\end{equation}
for convenience write $A_\lambda=A-\lambda \textbf{1}$ so our equation becomes
\begin{equation}
  A_\lambda \textbf{e}=\textbf{0}
\end{equation}
Now if we invert the matrix $A_\lambda$ everything goes wrong:
\begin{equation}
  \textbf{e}=A_\lambda^{-1}\textbf{0}=\textbf{0}
\end{equation}
so for the eigenvector to be something interesting, that is not just zero, the matrix $A_\lambda$ must have no inverse! Luckily we know how to check that, it means
\begin{equation}
  \det{A_\lambda}=0
\end{equation}
which we'll see in a second is just an equation for $\lambda$; this equation is called the \textsl{characteristic equation}.

Let's try this for a simple example, I amn't going to use the matrix
above, just because it turns out to have $\sqrt{2}$'s in its
eigenvalues, it isn't hard but lets keep things as straightforward as
possible and instead lets look at the example:
\begin{equation}
  A=
\begin{pmatrix}
2 & 1 \\
1 & 2
\end{pmatrix}
\end{equation}
so
\begin{equation}
  A_\lambda=
\begin{pmatrix}
2-\lambda & 1 \\
1 & 2-\lambda
\end{pmatrix}
\end{equation}
then
\begin{equation}
  \det{A_\lambda}=(2-\lambda)^2-1=\lambda^2-4\lambda+3
\end{equation}
so the characteristic equation is
\begin{equation}
  \lambda^2-4\lambda+3=0
\end{equation}
which has solutions $\lambda=3$ and $\lambda=1$.

It turns out once you know the eigenvalues you can easily work out the
eigenvectors by substituting back into the original equation. You can look up how to do this, it isn't hard. In this case for $\lambda=3$ the eigenvector is
\begin{equation}
  \textbf{e}=\begin{pmatrix} 1\\1\end{pmatrix}
\end{equation}
and for $\lambda=1$
\begin{equation}
  \textbf{e}=\begin{pmatrix} 1\\-1\end{pmatrix}
\end{equation}

There is a small subtlety here which is basically that an eigenvector
is a direction not a vector, if $\mathbf{e}$ is an eigenvector then so
is $\mu\mathbf{e}$ for any non-zero $\mu$, this hinges on the
linearity of matrices, say $A\mathbf{e}=\lambda\mathbf{e}$ then
$A(\mu\mathbf{e})=\mu
A\mathbf{e}=\mu\lambda\mathbf{e}=\lambda(\mu\mathbf{e})$, so if
$\mathbf{e}$ is an eigenvector, so is $\mu\mathbf{e}$.

\subsection*{Diagonalization}

What good are eigenvectors? Well, as I said before, you can think of
matrices as being made up of their eigenvectors; I don't want to go
into this in detail, there are a lot of details, but the example I
want to look at is diagonalization, this is a related sort of idea and
then one we need for doing PCA!

To talk about diagonalization, I am going to assume the matrix is
symmetric. This isn't required, there are diagonalizable non-symmetric
matrices, but the example we need for PCA is symmetric and the
symmetric case lacks all the caveats that we need for the more general
case, by focusing on diagonalizable we get to avoid some ``as long as
blah blah blah'' type statements.

To remind you about symmetry, the transpose of a matrix involves flipping around the diagonal, so if $A=[a_{ij}]$ then $A^T=[a_{ji}]$, for $2\times 2$ is
\begin{equation}
  A=
\begin{pmatrix}
a & c \\
d & b
  \end{pmatrix}
\end{equation}
then
\begin{equation}
  A^T=
\begin{pmatrix}
a & d \\
c & b
  \end{pmatrix}
\end{equation}
A matrix is symmetric if $A=A^T$ so
\begin{equation}
  A=
\begin{pmatrix}
1 & 2 \\
2 & 3
  \end{pmatrix}
\end{equation}
is symmetric,
\begin{equation}
  A=
\begin{pmatrix}
1 & 2 \\
-2 & 1
  \end{pmatrix}
\end{equation}
is not.

Now, what do we mean by diagonalization, it basically means that we can rewrite a symmetric matrix in the form
\begin{equation}
  A=PDP^{-1}
\end{equation}
where $D$ is a diagonal matrix made up of the eigenvalues:
\begin{equation}
  D=\text{diag}(\lambda_1,\lambda_2,\ldots,\lambda_n)
\end{equation}
and the $P$'s are matrices you make out of the eigenvalues.

This is both deep and useful; to deal with the useful aspect first, if
\begin{equation}
  A=PDP^{-1}
\end{equation}
then
\begin{equation}
  A^2=PDP^{-1}PDP^{-1}=PD^2P^{-1}
\end{equation}
and, diagonal matrices are easy to deal with, if
$D=\text{diag}(d_1,d_2,\ldots,d_n)$ then
$D^2=\text{diag}(d_1^2,d_2^2,\ldots,d_n^2)$. In this way diagonalizing
a matrix can make lots of things you might want to do with the matrix
easier.

The deep part is harder to see but it says in a way that for any
matrix there is a set of axes where the matrix just scales along the
axes. $P^{-1}$ is a matrix, so it makes a linear transformation on
your data, like a change of axes, so the equation is saying, first use
$P^{-1}$ to changes axes, then use $D$ to scale the axes and then use
$P$ to go back to your original axes: every matrix has a system of
axes where all it does is scale things!

In actual computational terms, we make $P$ using the eigenvectors as
columns, so $P=[\mathbf{e}_1,\mathbf{e}_2,\ldots,\mathbf{e}_n$. As we
pointed out there is some question of the length of the eigenvectors,
it is convenient in this context to make them unit length. There are
technical details here that I am sort of hiding but are actually
interesting and fascinating, basically for a symmetric matrix with an
inverse, the example we are dealing with here, the eigenvectors are
orthogonal, lets first make them have unit length so:
\begin{equation}
|\mathbf{e}_i|=1
\end{equation}
for all $i$, now
\begin{equation}
\mathbf{e}_i^{\mathsf T}\mathbf{e}_j=\delta_{ij}
\end{equation}
where I amn't explaining what happens if two eigenvectors have the
same eigenvalue, in that case you can make the eigenvectors
orthogonal, they aren't automatically orthogonal as they are when the
eigenvalues are different; again these are details you can read
about. The key thing is that in this case $P$ has the special property
that its transpose is its inverse:
\begin{equation}
P^{\mathsf T}P=\mathbf{1}
\end{equation}
As a point of terminology this means $P$ is an \textsl{orthogonal}
matrix.  You can check why that might be the case if you want. Another
point to learn more about if you want is that an orthogonal matrix
represents a rotation, or a rotation and a reflection, so
\begin{equation}
\mathbf{y}=P\mathbf{x}
\end{equation}
means that $\mathbf{y}$ is a rotation of $\mathbf{x}$, or a rotation and reflection.

Anyhow, for the example above then:
  \begin{equation}
    P=\frac{1}{\sqrt{2}}\begin{pmatrix} 1 & 1 \\ 1 & -1\end{pmatrix}
  \end{equation}
  and
\begin{equation}
\begin{pmatrix}
2 & 1\\
1 & 2
\end{pmatrix}
=
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & 1\\
1 & -1
\end{pmatrix}
\begin{pmatrix}
3 & 0\\
0 & 1
\end{pmatrix}
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & 1\\
1 & -1
\end{pmatrix}^{\mathsf T}
\end{equation}
  
\subsection*{A first look at PCA}

\begin{figure}[h]
  \centering
  \includegraphics[width=6cm]{anisotropic_gaussian.png}
  \caption{Anisotropic Gaussian with principal axes $(1,1)$ and
    $(1,-1)$. You can come up with whatever story you like about these
    data, lets say that the ear size of lots of people has been
    measured, the $x$ axis is the size of the left ear, the $y$ axis
    the size of the right ear. For simplicity the average has been
    removed. Now, clearly  the $x=y$ direction measures ``how big is the person'' and the $x=-y$ measures ``how symmetric is the person''.\label{fig:aniso}}
\end{figure}

In Figure~\ref{fig:aniso} we have data points with a clear structure,
they are spread out along the $x=y$ axis and have a bit of
``blurring'' along the $x=-y$ direction. In an experiment it might be
that the $x=y$ represented the phenomena, and $x=-y$ noise, you might
actually want to ignore the $x=-y$ direction and just project the
points onto the $x=y$ direction; in any case, discovering this
structure would be interesting. That's the business of PCA.

The first thing we might want to examine is the covariance matrix for the data:
\begin{equation}
\Sigma
=
\mathbb{E}\!\left[(x-\mu)(x-\mu)^{\mathsf T}\right].
\end{equation}
where $\mu$ is the mean and in our example we've already gotten rid of
that. In practice we work out the sample covariance:
\begin{equation}
\hat{\Sigma}_{ij}
=
\frac{1}{n-1}
\sum_{k=1}^{n}
\bigl(x_i^{(k)}-\bar{x}_i\bigr)
\bigl(x_j^{(k)}-\bar{x}_j\bigr).
\end{equation}
The little hat is there to distinguish the theoretical concept with
expectation values and what we actually work out from data.

For the data in Fig.~\ref{fig:aniso} this give
\begin{equation}
  \hat{\Sigma}=\begin{pmatrix}
  1.20532775& 0.92028907\\
0.92028907& 1.12193513
  \end{pmatrix}
\end{equation}
and so the 1.20 is the variance in the $x$ direction, 1.12 in the $y$
direction and the 0.92 is how much does $x$ depend on $y$. The data we
measure, $x$ and $y$ is covariant, what we are interested in
discovering, which in this simple example is clear from the graph, is
the direction $x=y$ where the real action is happening; the data is
something like ``every data point has some value that determines how
big $x$ and $y$ both are, except there is a little bit of noise that
makes them a bit different''. In this simple example the noise is
completely unrelated to the signal, so knowing how far you are in the
$x=y$ direction does not tell you anything about what is happening in
the $x=-y$ direction. For real data this is not often true, but it is
often approximately true. In any case, we can see that we are looking
for directions where the covariance matrix is diagonal! This is
diagonalization, what we looked at before.

If we work out the characteristic equation of $\Sigma$ above we get
$\lambda_1=2.07$ with eigenvector $\mathbf{e}_1=(1,1)^T$ and
$\lambda_2=0.26$ with eigenvector $\mathbf{e}_2=(1,-1)^T$, so, basically
\begin{equation}
  \hat{\Sigma}=\begin{pmatrix}
  1.20532775& 0.92028907\\
0.92028907& 1.12193513
  \end{pmatrix}= \frac{1}{2}
  \begin{pmatrix}
1 & 1\\
1 & -1
\end{pmatrix}
\begin{pmatrix}
2.07 & 0\\
0 & 0.26
\end{pmatrix}
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & 1\\
1 & -1
\end{pmatrix}^{\mathsf T}
\end{equation}

So what's happened here: basically if we have already removed the average, the covariance matrix is
\begin{equation}
\Sigma_\mathbf{x}
=
\mathbb{E}\!\left[\mathbf{x}\mathbf{x}^{\mathsf T}\right].
\end{equation}
What happens if we do a change of variables, imagine $\mathbf{x}=P\mathbf{y}$
\begin{equation}
\begin{aligned}
  \Sigma_{\mathbf{x}}
&=
\mathbb{E}\!\left[\mathbf{x}\mathbf{x}^{\mathsf T}\right]\\
&=
\mathbb{E}\!\left[P\mathbf{y}\mathbf{y}^{\mathsf T}P^{\mathsf T}\right]=P\mathbb{E}\!\left[\mathbf{y}\mathbf{y}^{\mathsf T}\right]P^{\mathsf T}\\
&=P\Sigma_{\mathbf{y}}P^{\mathsf T}
\end{aligned}
\end{equation}
In otherwords, diagonalizing the covariance matrix is just a way to
find new axes whose covariance matrix is diagonal. These new axes are
the \textsl{principal components}.

\subsection*{The 2024 Irish Election}

Elections to the D\'{a}il, the Irish parliament uses the greatest
voting system every created: single transferable vote. Clearly England
uses the worst, first past the post; which is obviously terrible and
not worth discussing.  The advantage of STV over the sort of list
systems employed in Europe is more subtle; a list system tries to
match the number of representative to the proportion of people voting
for them, if 25\% of people support party A then party A should have
roughly 25\% of the seats. STV does something better, it represents
the division inside the hearts of individuals, rather than the
division across the electorate; it is a beautiful idea, the make up of
the D\'{a}il represents not just how views change from person to person,
but the conflicting views inside each individual. I urge you to look
at how STV works.

We are going to ignore that for now though and look at only the first
preference vote, each person marks with a one the person they would
mostly like to see elected in their constituency. Ireland has 43
constituencies and elects 174 representatives, called TDs or Teachta\'{i}
D\'{a}la, to the D\'{a}il. There are a lot of parties, the two historic
parties of power Fianna F\'{a}il and Fine Gael, the insurgent Sinn F\'{e}in,
actually the oldest party in the state but never in power since it
lost the Irish Civil War in 1922, a few left wing parties, such as the
Labour Party, the Social Democrats and People before Profit, along
with some right wing parties such as Aont\'{u} and Independent Ireland;
many constituencies also have independent candidates who focus on
local issues, for example politics in Kerry is dominated by a single
family, the Healy-Raes who have few principles other than always
wearing cloth caps and trying to get money for Kerry.

There was an election in 2024; it saw the Green Party lose most of its
seats since it had annoyed its supporters by joining a coalition
government; Sinn F\'{e}in didn't do as well as had been expected and a
joint government formed of Fianna F\'{a}il and Fine Gael remained in
power, this coalition included some independents and dropped the Green
Party. All this means that the data for this election includes 43 data
points, one for each constituency and each data point is eleven
dimensional, corresponding to voter share for 10 political parties and
a final category of fringe parties and independents. Not every party
runs candidates in every constituency, for convenience the blank
entries are replaced with zeros.

If you do PCA on these data you end up with 11 eigenvalues, strongly
dominated by the first three or four, after that the get small. This
indicates that there are probably two or three main factors that
determine how people vote. By doing PCA, in this case using the
correlation matrix rather than the covariance data, this is a
technical detail made necessary by the fact the numbers always add up
to one. Fig.~\ref{fig:PC1vPC2} shows the result for the first and
second component coloured by the third.


\begin{figure}[h]
  \centering
  \includegraphics[width=15cm]{irish_ge_pca_pc1_pc2_pc3colour_12plotted.png}
  \caption{This shows the first three principal components, the first
    in the $x$-axis, the second, the $y$-axis and the third by
    colour. Only 12 data points, picked randomly, are shown otherwise
    it is hard to read. The $y$-axis is clearly a left-right axis,
    Dublin-Rathdown is a very liberal place, Cavan-Monaghan very
    conservative; the $x$-axis is not as clear but seems to measure
    diversity of opinion, the lefter points correspond to more complex
    places mixing rural and urban areas, for example. The third
    component, illustrated by colour, is interesting, it seems to
    correspond to independent candidates doing well, the two blues
    places, Galway West and Tipperary North have strong local
    candidates, Seamus Healy in Tipperary North and Catherine
    Connolly, who subsequently ran successfully for the presidency, in
    Galway West.\label{fig:PC1vPC2}}
\end{figure}


\section*{Summary}

\section*{Glossary}
$\mathbf{1}$ and $\mathbf{0}$ these are an `abused notation', I am
relying on your common sense to guess what they mean in context,
$\mathbf{1}$ might mean
\begin{equation}
  \mathbf{1}=\text{diag}(1,1,\ldots,1)
\end{equation}
or it might be a vector with just ones, similarly $\mathbf{0}$ might be
a matrix with only zeros or it might be vector with only
zeros. Mathematics, the supposed exactest of sciences, is often filled
with `abused notation'; it is an interesting development that
mathematicians these days are creating a computer language approach
called \texttt{Leen} that allows you, or forces you, to write down
mathematics in an exact way, which is useful and good for mathematics,
but not for communicating mathematics where stopping all the time to
give definitions can make things very dry and boring to read.



\end{document}

