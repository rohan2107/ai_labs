{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCqOaZeu55YQ"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This worksheet covers material for k-nn and linear regression. You will write functions to implement evaluation metrics for classification and regression problems. You will:\n",
    " - Use library functions from scikit-learn (https://scikit-learn.org/stable/)\n",
    " - Use NumPy and matplotlib\n",
    " - Write and call functions in Python\n",
    " - Gain understanding of the evaluation metrics used.\n",
    "\n",
    "\n",
    "Scikit-learn (https://scikit-learn.org/stable/) is a Python library with a wide range of ML algorithms. We will be using some of these algorithms during this course, but we will also be looking at the principles behind the algorithms in order to understand these rather than simply applying functions from libraries. Scikit-learn is a useful library for many ML tasks, it is convenient for the content in the unit and it is important you gain experience of using it. These days, we tend to use a set of different libraries for ML, moving between them as the task dictates, Scikit-learn is in that set as is NumPy, these can sometimes appear a little clunky when compared to, for example, PyTorch, but they have the use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8A7tY4e55YR"
   },
   "source": [
    "\n",
    "# 0. Preliminaries\n",
    "\n",
    "The instructions below will help you create a Python environment suitable for the both AI and Text Analytics labs. Although the environment file includes all packages required for the Text Analytics labs, you can use the same setup for the AI labs as well. The AI labs will primarily rely on [Scikit-learn](https://scikit-learn.org/stable/user_guide.html), [Scipy](https://docs.scipy.org/doc/scipy/), [Numpy](https://numpy.org/doc/stable/) and [PyTorch](https://docs.pytorch.org/docs/stable/index.html), but having the full environment ensures compatibility across all sessions.  You may also choose to install only the essential AI libraries on your own device; however, using the shared Text Analytics environment is perfectly acceptable and will work for all AI lab activities. If you encounter any issues installing the environment on your own device, please use the lab machines in MVB 2.11.\n",
    "\n",
    "Start by cloning the [GitHub repository](https://github.com/uob-TextAnalytics/text_labs_public) containing the lab materials.\n",
    "\n",
    "We recommend using ```conda``` to create an environment with the correct versions of all required packages. You can install either Anaconda or Miniconda, both of which include the ```conda``` program.\n",
    "\n",
    "A .yml file is provided that lists all the packages and versions we have tested with these labs. You can use this file to create your environment as follows:\n",
    "\n",
    "    1. Open a terminal and navigate to the directory containing this notebook and the file ```crossplatform_environment.yml```.\n",
    "    Use the ```cd``` command to move between directories.\n",
    "    2. *For lab machines only (MVB 2.11 and QB 1.80)*: load the Anaconda module: ```module load anaconda/3-2024```\n",
    "    3. Run the conda program by typing ```conda env create -f crossplatform_environment.yml```, then answer any questions that appear on the command line.\n",
    "    4. Activate the environment by running the command ```conda activate text_analytics```.\n",
    "    5. Make kernel available in Jupyter: ```python -m ipykernel install --user --name=text_analytics```.\n",
    "    6. Relaunch Jupyter: shutdown any running instances, and then type ```jupyter lab``` into your command line.\n",
    "    7. Find this notebook and open it up again.\n",
    "    8. Go to the top menu and change the kernel: click on 'Kernel'--> 'Change kernel' --> text_analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiuXcfw355YS"
   },
   "source": [
    "# 1. Classification\n",
    "\n",
    "We firstly import NumPy and matplotlib as we will be using these throughout the worksheet. We use a 'magic' function `%matplotlib inline` to display plots in the worksheet.\n",
    "\n",
    "In this question you will use a toy dataset from scikit-learn. You will use functions from scikit-learn to load the data, divide it into training and testing sets, and then fit a simple classifier to the training set. You will then write functions to calculate accuracy, precision, and recall. Finally, you will check your functions against the functions from scikit-learn.\n",
    "\n",
    "## Part a) Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LzvcY9oo55YT"
   },
   "outputs": [],
   "source": [
    "# scikit-learn comes with a number of toy datasets (https://sklearn.org/datasets/index.html#toy-datasets)\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# Load the wine dataset from sklearn. You may want to take a look at the format of the dataset\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "# Save the datapoints into the variable X and the targets into the variable y\n",
    "X = wine.data\n",
    "y = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGWjdTm755YT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtQvEa_A55YT"
   },
   "source": [
    "Take a look at the target values in y. What do you notice about these? Why are these suitable for a classification algorithm rather than a regression algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Q54JJc0O55YT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#  Look at the values in y\n",
    "##TODO##\n",
    "print(np.unique(y))\n",
    "print(y[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z6w_IU155YT"
   },
   "source": [
    "## Part b) Divide the data into training and testing sets\n",
    "Use the function `train_test_split` from `sklearn.model_selection` to split out the data and targets into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sdcEme9f55YT"
   },
   "outputs": [],
   "source": [
    "# We import the function train_test_split from sklearn and use this to split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The function returns splits of each array passed in.\n",
    "# The proportion to be used as the training set is given by test_size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqV-jCyp55YT"
   },
   "source": [
    "## Part c) Import the k-nearest neighbours classifier and run it on the data\n",
    "Scikit-learn has a huge range of *estimators* that you can use with your dataset. An estimator is any procedure that can be used to fit data and make predictions from it. Here we will import the k-nearest neighbours classifier, instantiate it, run it on our training set, and then use it to generate some predictions. You will learn more about k-nearest neighbours in Week 14. For now, we are simply using it to generate some predictions.\n",
    "\n",
    "The general procedure for using the estimators in scikit-learn is as follows. Every estimator has a method `fit(X, y)` and a method `predict(T)`.\n",
    "\n",
    "1) Import the estimator\n",
    "    e.g. `from sklearn.models import Classifier`\n",
    "    \n",
    "2) Instantiate the estimator to a variable\n",
    "    e.g. `est = Classifier(hparams)`\n",
    "    \n",
    "3) Fit the estimator to the data\n",
    "    e.g. `est.fit(X, y)`\n",
    "    \n",
    "4) Make a prediction\n",
    "    e.g. `predictions = est.predict(test_data)`\n",
    "    \n",
    "You can see an example of this in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "S11MsHL855YU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We first import the classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# We instantiate the classifier with 5 neighbours\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# We fit the model using our training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Finally, we generate predictions on the test data\n",
    "ypred_test=knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J8MqMfC55YU"
   },
   "source": [
    "## Part d) Evaluating the classifier\n",
    "In this section we will implement functions for accuracy, precision and recall, and compare them with the functions given in sklearn (they should give the same results!)\n",
    "\n",
    "The wine dataset has 3 classes. We will write functions to compute the accuracy of the classifer, the macro-averaged precision and the macro-averaged recall.\n",
    "\n",
    "Recall the equations for accuracy, precision, and recall:\n",
    "\n",
    "$$\\text{accuracy} = \\frac{\\text{Number correct}}{\\text{Total datapoints}}$$\n",
    "i.e. the number of correctly classified datapoints as a proportion of all $n$ datapoints\n",
    "\n",
    "$$\\text{precision}(c) = \\frac{TP_c}{TP_c+FP_c}$$\n",
    "that is, the precision for class $c$ is the number of true positives for class $c$ as a proportion of the total number of positive predictions for class $c$\n",
    "\n",
    "$$\\text{recall}(c) = \\frac{TP_c}{TP_c+FN_c}$$\n",
    "that is, the recall for class $c$ is the number of true positives for class $c$ as a proportion of the total number of actual positives for class $c$\n",
    "\n",
    "The macro-averaged precision and macro-averaged recall are then simply calculated by averaging the precision (or recall) for each class:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{precision} &=& \\frac{1}{k} \\sum_{c = 1}^k \\text{precision}(c)\\cr\n",
    "   \\text{recall} &=& \\frac{1}{k} \\sum_{c = 1}^k \\text{recall}(c)\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can automatically generate the confusion matrix for our data using the function `confusion_matrix` from `sklearn.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lmY36WBU55YU"
   },
   "outputs": [],
   "source": [
    "# Import the function confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Build the confusion matrix from the target test set y_test and our predicted values ypred_test\n",
    "cm = confusion_matrix(y_test, ypred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBMLDa3b55YU"
   },
   "source": [
    "Take a look at the confusion matrix. What should its dimensions be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8-qTpn5855YU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  2  1]\n",
      " [ 1 12  0]\n",
      " [ 0  7  4]]\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "# Look at the confusion matrix cm\n",
    "##TODO##\n",
    "print(cm)\n",
    "print(cm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73jSZdAt55YU"
   },
   "source": [
    "(**Optional**) Write a function `my_accuracy` that takes in two arrays `y` for target values and `pred` for predicted  values, and returns accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MPOhLu6U55YU"
   },
   "outputs": [],
   "source": [
    "def my_accuracy(y, pred):\n",
    "    # Write your answer here\n",
    "    acc=np.mean(y == pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wxf3nlVJ55YV"
   },
   "source": [
    "(**Optional**) Write a function `my_recall_macro` that takes in two arrays `y` for target values and `pred` for predicted  values, and returns recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "72GU9RGu55YV"
   },
   "outputs": [],
   "source": [
    "def my_recall_macro(y, pred):\n",
    "    recalls = []\n",
    "    cm = confusion_matrix(y, pred)\n",
    "    # Write your answer here\n",
    "    for c in range(cm.shape[0]):\n",
    "        TP = cm[c, c]\n",
    "        FN = np.sum(cm[c, :]) - TP\n",
    "        recalls.append(TP / (TP + FN))\n",
    "    return np.mean(recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fusqxa7o55YV"
   },
   "source": [
    "(**Optional**) Write a function `my_precision_macro` that takes in two arrays `y` for target values and `pred` for predicted  values, and returns precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MKk-7KB555YV"
   },
   "outputs": [],
   "source": [
    "def my_precision_macro(y, pred):\n",
    "    # Write your answer here\n",
    "    cm = confusion_matrix(y, pred)\n",
    "    precisions = []\n",
    "\n",
    "    for c in range(cm.shape[0]):\n",
    "        TP = cm[c, c]\n",
    "        FP = np.sum(cm[:, c]) - TP\n",
    "        precisions.append(TP / (TP + FP))\n",
    "\n",
    "    return np.mean(precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kj7n2WR055YV"
   },
   "source": [
    "(**Optional**) Check that your functions match those in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "u8v42Ms155YV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "my_accuracy(y_test, ypred_test) == accuracy_score(y_test, ypred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jMqqYk1D55YV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_recall_macro(y_test, ypred_test)==recall_score(y_test, ypred_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jLD2Rq3O55YV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_precision_macro(y_test, ypred_test)==precision_score(y_test, ypred_test, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "text_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
